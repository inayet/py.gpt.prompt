@startuml ChatModel
!theme blueprint

' docs/diagrams/plantuml/chat_model.puml

class ConfigurationManager extends Singleton {
    // JSONMap = Dict[str, Any]
    - MappingTemplate _map_template
    + ConfigurationManager(file_path: str, initial_data: Optional[JSONMap])
    + bool load()
    + bool save()
    + bool backup()
    + Any get_value(key: str, default: Optional[Any])
    + bool set_value(key: str, value: Any)
    + Optional[str] evaluate_path(key: str, default: Optional[Any])
    + str get_environment(variable: str)
}

class ChatCompletionMessage extends TypeDict {
    + role: Literal["assistant", "user", "system", "function"]
    + content: NotRequired[str]
    + function_call: NotRequired[str]
    + function_args: NotRequired[str]
    + user: NotRequired[str]
}

abstract class ChatModel {
    // ChatModelVector = Union[List[int], List[float]]
    // ChatModelEncoding = ChatModelVector
    // ChatModelEmbedding = List[ChatModelVector]
    // ChatModelDocument = str
    // ChatModelDocuments = List[ChatModelDocument]
    // ChatModelTextCompletion = str
    - ConfigurationManager _config
    + ChatModel(config: ConfigurationManager)
    + ChatModelTextCompletion get_completion(prompt: str)
    + ChatCompletionMessage get_chat_completion(messages: List[ChatCompletionMessage])
    + ChatModelEmbedding get_embedding(input: Union[str, List[str]])
    + ChatModelEncoding get_encoding(text: str)
}

abstract class EmbeddingFunction {
    - ChatModelEmbedding __call__(texts: ChatModelDocuments)
}

class ChatModelEmbeddingFunction extends EmbeddingFunction {
    + ChatModelEmbeddingFunction(model: ChatModel)
}

class ChatModelFactory {
    // Provider is one of: openai, llama, llama_cpp, etc...
    - ConfigurationManager _config
    - Dict[str, Callable] _provider_map
    + ChatModelFactory(config: ConfigurationManager)
    + ChatModel create_model(provider: str)
}

class FunctionFactory {
    - ConfigurationManager _config
    - dict[str, Callable] _functions
    - str _function_name
    - dict[str, Any] _function_args
    - Optional[Callable] _function
    + FunctionFactory(config: ConfigurationManager)
    + dict[str, Any] get_function_args(message: ChatCompletionMessage)
    + Optional[Callable] get_function(message: ChatCompletionMessage)
    + None register_function(function_name: str, function: Callable)
    + Optional[ChatCompletionMessage] execute_function(message: ChatCompletionMessage)
    + Optional[ChatCompletionMessage] query_function(model: ChatModel, result: ChatCompletionMessage, messages: List[ChatCompletionMessage])
}

ChatModelFactory --> ChatModel : "creates"
ChatModelFactory *-- ConfigurationManager

ChatModel *-- ConfigurationManager
ChatModel --> ChatCompletionMessage : "uses"
' note left of ChatModel: Maximum token limit in sequence is model-dependent. Refer to model documentation for specifics.

FunctionFactory *-- ConfigurationManager
FunctionFactory *-- Callable
FunctionFactory --> ChatCompletionMessage : "uses"
FunctionFactory <-- ChatModel : "uses"

ChatModelEmbeddingFunction --> ChatModel : "uses"
@enduml