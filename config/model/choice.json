{
  "providers": ["huggingface", "openai"],
  "huggingface": {
    "choices": {
      "device_types": ["cpu", "cuda", "opencl", "vulkan", "hip", "mps", "lazy"],
      "embeddings_classes": [
        "HuggingFaceEmbeddings",
        "HuggingFaceInstructEmbeddings",
        "OpenAIEmbeddings",
        "LlamaCppEmbeddings"
      ],
      "embeddings_models": [
        "hkunlp/instructor-base",
        "hkunlp/instructor-large",
        "hkunlp/instructor-xl",
        "sentence-transformers/all-MiniLM-L6-v2",
        "sentence-transformers/all-MiniLM-L12-v2"
      ],
      "repo_ids": [
        "TheBloke/orca_mini_3B-GGML",
        "TheBloke/orca_mini_7B-GGML",
        "TheBloke/orca_mini_13B-GGML"
      ]
    },
    "device_type": "cpu",
    "embeddings": {
      "repository": "hkunlp/instructor-large",
      "class": "HuggingFaceInstructEmbeddings"
    },
    "models": {
      "orca_mini_7B-GGML": {
        "type": "completions",
        "repository": "TheBloke/orca_mini_7B-GGML",
        "filename": "orca-mini-7b.ggmlv3.q4_0.bin",
        "max_tokens": 512,
        "max_context": 2048
      }
    }
  },
  "openai": {
    "embeddings": {
      "model": "text-embedding-ada-002",
      "max_token_limit": 8192
    },
    "models": {
      "gpt-3.5-turbo": {
        "type": "chat_completions",
        "model": "gpt-3.5-turbo",
        "max_tokens": 1024,
        "max_context": 4096
      },
      "gpt-3.5-turbo-16k": {
        "type": "chat_completions",
        "model": "gpt-3.5-turbo-16k",
        "max_tokens": 2048,
        "max_context": 16384
      },
      "gpt-4": {
        "type": "chat_completions",
        "model": "gpt-4",
        "max_tokens": 1024,
        "max_context": 8192
      },
      "gpt-4-32k": {
        "type": "chat_completions",
        "model": "gpt-4-32k",
        "max_tokens": 1024,
        "max_context": 32768
      }
    }
  }
}
